{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Modern AI Pro: Types of memory\n",
        "We will use different types of memory to store our conversations."
      ],
      "metadata": {
        "id": "RyNVRd6-6_U2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_134HQc5lqlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e188c83-b924-4350-e789-0e99d3e9f094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for mitrallm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[1mMitraLLM\u001b[0m\n",
            "Params: {'URL': 'https://mitrallm.mitrarobot.com'}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "!pip install git+https://gitlab.com/gauthammsam/mitrallm.git --quiet\n",
        "from mitrallm import MitraLLM\n",
        "llm = MitraLLM(\n",
        "    token     = userdata.get(\"MITRA_TOKEN\"),\n",
        "    accessapi = userdata.get(\"MITRA_ENDPOINT\")\n",
        ")\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic: Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "LfkqRf7V7JIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.load_memory_variables({})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMuMeV9A3lPO",
        "outputId": "7a61a5bd-d811-462c-a5b0-21f27312030e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful chatbot\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "HZVCI8qM4ACy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "8z1wrVD43x7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "input = {\"input\": \"I want to understand more about AI\"}\n",
        "response = chain.invoke({\"input\":input }, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MkBr0YHZ4PCj",
        "outputId": "a0c56b25-ae0d-4b58-ca7d-1cf3a052fada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence > 5:chain:load_memory_variables] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  }\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence > 5:chain:load_memory_variables] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"history\": []\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence > 6:chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"history\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence > 6:chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history> > 4:chain:RunnableSequence] [3ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history> > 3:chain:RunnableParallel<history>] [10ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"history\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableAssign<history>] [14ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  },\n",
            "  \"history\": []\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"input\": \"I want to understand more about AI\"\n",
            "  },\n",
            "  \"history\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"SystemMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"You are a helpful chatbot\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"{'input': 'I want to understand more about AI'}\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:MitraLLM] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful chatbot\\nHuman: {'input': 'I want to understand more about AI'}\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:MitraLLM] [1.17s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"System: Sure, I'd be happy to help! AI stands for artificial intelligence\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.19s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"System: Sure, I'd be happy to help! AI stands for artificial intelligence\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"System: Sure, I'd be happy to help! AI stands for artificial intelligence\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(input, {\"output\": response})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSHUCPji4ZKF",
        "outputId": "b6359875-bdb3-4468-d651-d13ba2c41de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='I want to understand more about AI'),\n",
              "  AIMessage(content=\"System: Sure, I'd be happy to help! AI stands for artificial intelligence\")]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Summary Memory"
      ],
      "metadata": {
        "id": "0Q1Hqwu87T9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "summary_memory.load_memory_variables({})\n",
        "\n",
        "long_response = \"\"\"Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\n",
        "complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\n",
        "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
        "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
        "\n",
        "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
        "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\n",
        "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
        "Feedback (RLHF). Although the training methodology is simple, high computational requirements have\n",
        "limited the development of LLMs to a few players.\n",
        "\n",
        "There have been public releases of pretrained LLMs\n",
        "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\n",
        "match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n",
        "(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\n",
        "as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\n",
        "preferences, which greatly enhances their usability and safety. This step can require significant costs in\n",
        "compute and human annotation, and is often not transparent or easily reproducible, limiting progress within\n",
        "the community to advance AI alignment research.\n",
        "\n",
        "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\n",
        "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
        "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
        "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
        "Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\n",
        "annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\n",
        "this paper contributes a thorough description of our fine-tuning methodology and approach to improving\n",
        "LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
        "continue to improve the safety of those models, paving the way for more responsible development of LLMs.\n",
        "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\n",
        "the emergence of tool usage and temporal organization of knowledge.\"\"\""
      ],
      "metadata": {
        "id": "ObGL6CcK63Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_memory.save_context({\"input\": \"Tell me about Llama 2\"}, {\"output\": long_response})\n",
        "summary_memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTA14_8K76h-",
        "outputId": "9dd7bc57-d8f7-4ad2-f221-a09b353437c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'The AI explains that Large Language Models (LLMs) like Llama 2'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationKGMemory\n",
        "kgmemory = ConversationKGMemory(llm=llm, return_messages=True)\n",
        "kgmemory.save_context({\"input\": \"say hi to sam\"}, {\"output\": \"who is sam\"})\n",
        "kgmemory.save_context({\"input\": \"sam is a friend\"}, {\"output\": \"okay\"})\n",
        "kgmemory.load_memory_variables({\"input\": \"who is sam\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOGvkWxJ83Fs",
        "outputId": "cce8d320-6a00-45f1-d5b0-cc6dc2fc772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='On Sam: Sam is a friend.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kgmemory.get_knowledge_triplets(\"her favorite color is red\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WM4x4obAbOw",
        "outputId": "78dbebd6-8b07-4815-9195-00948674e96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[KnowledgeTriple(subject='Sam', predicate='favorite color is', object_='red')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}